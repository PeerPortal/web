

### **æŠ€æœ¯æ–‡æ¡£ï¼šé«˜çº§AIç•™å­¦è§„åˆ’å¸ˆ (Powered by LangGraph & RAG)**

#### **é¡¹ç›®æ¦‚è¿°**

æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªé«˜çº§æ™ºèƒ½ä½“ï¼Œå®ƒä¸ä»…èƒ½ä¸Šç½‘æœç´¢ï¼Œè¿˜èƒ½**å­¦ä¹ ä½ ä¸Šä¼ çš„ç§æœ‰çŸ¥è¯†**ï¼ˆå¦‚PDF/DOCæ ¼å¼çš„ç•™å­¦æ¡ˆä¾‹ã€é™¢æ ¡å†…éƒ¨èµ„æ–™ï¼‰ï¼Œå¹¶æ‹¥æœ‰**å¯è·¨ä¼šè¯çš„é•¿æœŸè®°å¿†**ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ `LangGraph` æ¥æ„å»ºå…¶æ ¸å¿ƒçš„æ€è€ƒ-è¡ŒåŠ¨å¾ªç¯ï¼Œå¹¶ç”¨ `Streamlit` æ­å»ºä¸€ä¸ªå¯äº¤äº’çš„Webç•Œé¢ã€‚

**æ ¸å¿ƒæŠ€æœ¯æ ˆ:**

| ç»„ä»¶ | æŠ€æœ¯ | ä½œç”¨ |
| :--- | :--- | :--- |
| **æ™ºèƒ½ä½“æ ¸å¿ƒ** | **LangGraph** | æ„å»ºå¥å£®ã€å¯æ§çš„æ™ºèƒ½ä½“å·¥ä½œæµï¼ˆReActå¾ªç¯ï¼‰ã€‚ |
| **å¤§è¯­è¨€æ¨¡å‹** | OpenAI GPT-4 | è´Ÿè´£æ¨ç†ã€å†³ç­–å’Œç”Ÿæˆã€‚ |
| **Webæœç´¢å·¥å…·** | Tavily AI | æä¾›ä¸“ä¸ºAIä¼˜åŒ–çš„å®æ—¶ç½‘ç»œæœç´¢èƒ½åŠ›ã€‚ |
| **æ–‡ä»¶å¤„ç†** | `unstructured`, `pypdf` | è§£æç”¨æˆ·ä¸Šä¼ çš„PDFã€DOCç­‰æ–‡ä»¶å†…å®¹ã€‚ |
| **çŸ¥è¯†åº“/é•¿æœŸè®°å¿†** | **ChromaDB** + OpenAI Embeddings | å°†æ–‡ä»¶å†…å®¹å‘é‡åŒ–åå­˜å…¥ChromaDBï¼Œå®ç°é•¿æœŸè®°å¿†å’ŒRAGæ£€ç´¢ã€‚ |
| **çŸ­æœŸè®°å¿†** | LangChain Memory | åœ¨å•æ¬¡ä¼šè¯ä¸­ç»´æŒä¸Šä¸‹æ–‡ã€‚ |
| **Webç•Œé¢** | **Streamlit** | å¿«é€Ÿæ­å»ºä¸€ä¸ªæ”¯æŒæ–‡ä»¶ä¸Šä¼ å’Œå®æ—¶å¯¹è¯çš„äº¤äº’å¼Web UIã€‚ |
| **åç«¯æœåŠ¡** | **FastAPI** | ï¼ˆå¯é€‰é›†æˆï¼‰å°†Agenté€»è¾‘å°è£…æˆAPIï¼Œä¾›æ›´å¤æ‚çš„ç³»ç»Ÿè°ƒç”¨ã€‚æœ¬æ•™ç¨‹å°†Streamlitä½œä¸ºä¸»æœåŠ¡ã€‚ |

-----

### **ç¬¬ä¸€æ­¥ï¼šé¡¹ç›®è®¾ç½®ä¸ç¯å¢ƒå‡†å¤‡**

1.  **åˆ›å»ºé¡¹ç›®ç»“æ„:**

    ```
    /advanced_study_agent
    |-- /knowledge_base           # å­˜æ”¾ä¸Šä¼ çš„PDF/DOCæ–‡ä»¶
    |-- /vector_store             # å­˜æ”¾ChromaDBçš„æŒä¹…åŒ–æ•°æ®
    |-- app/
    |   |-- agent_state.py        # å®šä¹‰LangGraphçš„çŠ¶æ€
    |   |-- agent_tools.py        # å®šä¹‰æ‰€æœ‰å·¥å…· (æœç´¢, RAG)
    |   |-- agent_graph.py        # æ„å»ºLangGraphçš„æ ¸å¿ƒé€»è¾‘
    |-- config.py                 # å­˜æ”¾é…ç½®å’ŒAPIå¯†é’¥
    |-- app.py                    # Streamlitåº”ç”¨ä¸»æ–‡ä»¶
    |-- requirements.txt          # é¡¹ç›®ä¾èµ–
    ```

2.  **å®‰è£…ä¾èµ–:**
    åˆ›å»º `requirements.txt` æ–‡ä»¶å¹¶å†™å…¥ä»¥ä¸‹å†…å®¹ï¼š

    ```txt
    langchain
    langchain-openai
    langgraph
    langchain_community
    streamlit
    fastapi
    uvicorn
    tavily-python
    chromadb
    unstructured[docx,pdf]
    pypdf
    python-dotenv
    ```

    ç„¶åé€šè¿‡pipå®‰è£…ï¼š

    ```bash
    pip install -r requirements.txt
    ```

3.  **è®¾ç½®APIå¯†é’¥:**
    åˆ›å»º `.env` æ–‡ä»¶ï¼Œå¹¶å¡«å…¥ä½ çš„å¯†é’¥ï¼š

    ```env
    OPENAI_API_KEY="sk-..."
    TAVILY_API_KEY="tvly-..."
    ```

    åœ¨ `config.py` ä¸­åŠ è½½è¿™äº›å¯†é’¥ï¼š

    ```python
    # config.py
    import os
    from dotenv import load_dotenv

    load_dotenv()

    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
    ```

-----

### **ç¬¬äºŒæ­¥ï¼šæ„å»ºçŸ¥è¯†åº“ä¸è®°å¿†ç³»ç»Ÿ (RAG + ChromaDB)**

è¿™æ˜¯å®ç°Agentâ€œå­¦ä¹ â€èƒ½åŠ›å’Œé•¿æœŸè®°å¿†çš„å…³é”®ã€‚

1.  **æ–‡ä»¶å¤„ç†ä¸å‘é‡åŒ–:**
    æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå®ƒèƒ½è¯»å–æ–‡ä»¶å¤¹ä¸­çš„æ–‡æ¡£ï¼Œå°†å…¶åˆ†å‰²æˆå°å—ï¼ˆChunksï¼‰ï¼Œç„¶åä½¿ç”¨OpenAIçš„Embeddingæ¨¡å‹è¿›è¡Œå‘é‡åŒ–ï¼Œæœ€åå­˜å…¥ChromaDBã€‚

    ```python
    # åœ¨ app/agent_tools.py æˆ–ä¸€ä¸ªæ–°æ–‡ä»¶ knowledge_base.py ä¸­
    import os
    from langchain_community.document_loaders import DirectoryLoader
    from langchain_community.vectorstores import Chroma
    from langchain_openai import OpenAIEmbeddings
    from langchain.text_splitter import RecursiveCharacterTextSplitter

    VECTOR_STORE_PATH = "./vector_store"
    KNOWLEDGE_BASE_PATH = "./knowledge_base"

    def load_and_embed_knowledge_base():
        """åŠ è½½ã€åˆ†å‰²ã€åµŒå…¥å¹¶å­˜å‚¨çŸ¥è¯†åº“æ–‡æ¡£ã€‚"""
        # 1. åŠ è½½æ–‡æ¡£
        loader = DirectoryLoader(KNOWLEDGE_BASE_PATH, glob="**/*.(pdf|docx|doc)", show_progress=True)
        documents = loader.load()

        if not documents:
            print("çŸ¥è¯†åº“ä¸ºç©ºï¼Œè·³è¿‡åµŒå…¥è¿‡ç¨‹ã€‚")
            return None

        # 2. åˆ†å‰²æ–‡æ¡£
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(documents)

        # 3. åˆ›å»ºå¹¶æŒä¹…åŒ–å‘é‡æ•°æ®åº“
        print(f"æ­£åœ¨åˆ›å»ºå¹¶æŒä¹…åŒ–å‘é‡æ•°æ®åº“ï¼Œå…± {len(splits)} ä¸ªæ–‡æ¡£å—...")
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=OpenAIEmbeddings(),
            persist_directory=VECTOR_STORE_PATH
        )
        print("å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆã€‚")
        return vectorstore

    def get_retriever():
        """è·å–ç°æœ‰çš„å‘é‡æ•°æ®åº“æ£€ç´¢å™¨ã€‚"""
        if not os.path.exists(VECTOR_STORE_PATH):
            return load_and_embed_knowledge_base().as_retriever()
        
        vectorstore = Chroma(
            persist_directory=VECTOR_STORE_PATH,
            embedding_function=OpenAIEmbeddings()
        )
        return vectorstore.as_retriever(search_kwargs={"k": 3}) # è¿”å›æœ€ç›¸å…³çš„3ä¸ªç»“æœ

    ```

2.  **å®šä¹‰çŸ¥è¯†åº“æ£€ç´¢å·¥å…· (`app/agent_tools.py`)**
    ç°åœ¨ï¼Œæˆ‘ä»¬æŠŠæ£€ç´¢å™¨å°è£…æˆä¸€ä¸ªAgentå¯ä»¥è°ƒç”¨çš„å·¥å…·ã€‚

    ```python
    # app/agent_tools.py
    from langchain.tools import tool
    from typing import List

    # (æ¥ä¸Šæ–‡çš„ get_retriever å‡½æ•°)

    @tool
    def knowledge_base_retriever(query: str) -> List[str]:
        """
        å½“éœ€è¦å›ç­”å…³äºç•™å­¦ç”³è¯·ç­–ç•¥ã€ç‰¹å®šå­¦æ ¡çš„å†…éƒ¨ä¿¡æ¯ã€æ–‡ä¹¦å†™ä½œæŠ€å·§æˆ–è¿‡å¾€æˆåŠŸæ¡ˆä¾‹æ—¶ï¼Œä½¿ç”¨æ­¤å·¥å…·ã€‚
        æ­¤å·¥å…·èƒ½ä»å¹³å°çš„ç§æœ‰çŸ¥è¯†åº“ä¸­æ£€ç´¢æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚
        """
        retriever = get_retriever()
        if retriever is None:
            return ["çŸ¥è¯†åº“ä¸ºç©ºï¼Œæ— æ³•æŸ¥è¯¢ã€‚"]
        
        docs = retriever.invoke(query)
        return [doc.page_content for doc in docs]
    ```

-----

### **ç¬¬ä¸‰æ­¥ï¼šå®šä¹‰Agentçš„å·¥å…·é›†ä¸æ ¸å¿ƒé€»è¾‘ (LangGraph)**

1.  **æ·»åŠ å…¶ä»–å·¥å…· (`app/agent_tools.py`)**

    ```python
    # app/agent_tools.py (ç»§ç»­æ·»åŠ )
    from langchain_community.tools.tavily_search import TavilySearchResults

    # 1. ç½‘ç»œæœç´¢å·¥å…·
    web_search_tool = TavilySearchResults(max_results=3, name="web_search")

    # 2. å°†æ‰€æœ‰å·¥å…·æ”¾å…¥ä¸€ä¸ªåˆ—è¡¨
    agent_tools = [web_search_tool, knowledge_base_retriever]
    ```

2.  **å®šä¹‰AgentçŠ¶æ€ (`app/agent_state.py`)**
    `LangGraph` çš„æ ¸å¿ƒæ˜¯çŠ¶æ€æœºã€‚æˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªè´¯ç©¿æ•´ä¸ªæµç¨‹çš„çŠ¶æ€å¯¹è±¡ã€‚

    ```python
    # app/agent_state.py
    from typing import List, TypedDict
    from langchain_core.messages import BaseMessage

    class AgentState(TypedDict):
        # inputæ˜¯ç”¨æˆ·çš„åŸå§‹è¾“å…¥
        input: str
        # chat_historyæ˜¯çŸ­æœŸè®°å¿†
        chat_history: List[BaseMessage]
        # agent_outcomeæ˜¯Agentæ‰§è¡Œçš„ç»“æœ
        agent_outcome: dict
        # intermediate_stepsæ˜¯å·¥å…·è°ƒç”¨çš„ä¸­é—´è¿‡ç¨‹
        intermediate_steps: list
    ```

3.  **æ„å»ºAgentå›¾ (`app/agent_graph.py`)**
    è¿™æ˜¯æœ€æ ¸å¿ƒçš„éƒ¨åˆ†ã€‚æˆ‘ä»¬å°†å®šä¹‰å›¾çš„èŠ‚ç‚¹ï¼ˆNodesï¼‰å’Œè¾¹ï¼ˆEdgesï¼‰ã€‚

    ```python
    # app/agent_graph.py
    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
    from langchain.agents import create_tool_calling_agent, AgentExecutor
    from langgraph.graph import StateGraph, END
    from langgraph.prebuilt import ToolNode

    from app.agent_state import AgentState
    from app.agent_tools import agent_tools
    import config

    # 1. åˆå§‹åŒ–LLM
    llm = ChatOpenAI(model="gpt-4-turbo", openai_api_key=config.OPENAI_API_KEY, temperature=0)

    # 2. åˆ›å»ºAgentçš„æ ¸å¿ƒé€»è¾‘ (LLM + Prompt + Tools)
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å‹å–„çš„AIç•™å­¦è§„åˆ’å¸ˆï¼Œåå«â€œå¯èˆªAIâ€ã€‚
        ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·çš„æé—®ï¼Œå†³ç­–æ˜¯ä½¿ç”¨ç½‘ç»œæœç´¢æ¥è·å–æœ€æ–°ä¿¡æ¯ï¼Œè¿˜æ˜¯æŸ¥è¯¢ç§æœ‰çŸ¥è¯†åº“æ¥è·å–ä¸“ä¸šç»éªŒå’Œæ¡ˆä¾‹ã€‚
        è¯·ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“ï¼Œå› ä¸ºé‚£é‡Œæœ‰æ›´ä¸“ä¸šçš„ã€ç»è¿‡éªŒè¯çš„ä¿¡æ¯ã€‚åªæœ‰åœ¨çŸ¥è¯†åº“æ— æ³•å›ç­”ï¼Œæˆ–è€…éœ€è¦æŸ¥è¯¢éå¸¸æœ‰æ—¶æ•ˆæ€§ï¼ˆæ¯”å¦‚ä»Šå¤©çš„æ–°é—»ã€è‚¡ä»·ï¼‰çš„ä¿¡æ¯æ—¶ï¼Œæ‰ä½¿ç”¨ç½‘ç»œæœç´¢ã€‚
        è¯·ç»“åˆçŸ­æœŸå¯¹è¯å†å²è¿›è¡Œå›ç­”ï¼Œä»¥ç¡®ä¿å¯¹è¯çš„è¿è´¯æ€§ã€‚"""),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])

    # è¿™ä¸ªagentè´Ÿè´£å†³ç­–
    decider_agent = create_tool_calling_agent(llm, agent_tools, prompt)

    # 3. å®šä¹‰å›¾çš„èŠ‚ç‚¹
    # AgentèŠ‚ç‚¹ï¼šè´Ÿè´£è°ƒç”¨LLMè¿›è¡Œå†³ç­–
    def run_agent_node(state: AgentState):
        agent_outcome = decider_agent.invoke({
            "input": state["input"],
            "chat_history": state["chat_history"],
            "intermediate_steps": state["intermediate_steps"]
        })
        return {"agent_outcome": agent_outcome, "intermediate_steps": []}

    # ToolèŠ‚ç‚¹ï¼šè´Ÿè´£æ‰§è¡Œå·¥å…·
    tool_node = ToolNode(agent_tools)

    # 4. å®šä¹‰å›¾çš„è¾¹ï¼ˆè·¯ç”±é€»è¾‘ï¼‰
    def should_continue(state: AgentState):
        if state["agent_outcome"].tool_calls:
            return "continue" # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œåˆ™ç»§ç»­
        else:
            return "end" # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼ˆä»£è¡¨å·²ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼‰ï¼Œåˆ™ç»“æŸ

    # 5. æ„å»ºå›¾
    graph = StateGraph(AgentState)

    graph.add_node("agent", run_agent_node)
    graph.add_node("action", tool_node)

    graph.set_entry_point("agent")

    graph.add_conditional_edges(
        "agent",
        should_continue,
        {
            "continue": "action",
            "end": END,
        },
    )

    graph.add_edge("action", "agent")

    # ç¼–è¯‘æˆå¯æ‰§è¡Œçš„åº”ç”¨
    agent_graph = graph.compile()
    ```

-----

### **ç¬¬å››æ­¥ï¼šæ„å»ºäº¤äº’å¼Webç•Œé¢ (`app.py`)**

æˆ‘ä»¬å°†ä½¿ç”¨ `Streamlit` æ¥åˆ›å»ºä¸€ä¸ªç”¨æˆ·å‹å¥½çš„ç•Œé¢ã€‚

```python
# app.py
import streamlit as st
from langchain_core.messages import AIMessage, HumanMessage
import os

from app.agent_graph import agent_graph
from app.agent_tools import load_and_embed_knowledge_base

st.set_page_config(page_title="AIç•™å­¦è§„åˆ’å¸ˆ-å¯èˆªAI", layout="wide")

st.title("AIç•™å­¦è§„åˆ’å¸ˆ - å¯èˆªAI ğŸš€")

# ---- æ–‡ä»¶ä¸Šä¼ ä¸çŸ¥è¯†åº“æ„å»º ----
with st.sidebar:
    st.header("çŸ¥è¯†åº“ç®¡ç†")
    uploaded_files = st.file_uploader(
        "è¯·ä¸Šä¼ PDFæˆ–DOCXæ–‡ä»¶",
        type=["pdf", "docx", "doc"],
        accept_multiple_files=True
    )
    if st.button("æ„å»º/æ›´æ–°çŸ¥è¯†åº“"):
        if uploaded_files:
            with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶ï¼Œè¯·ç¨å€™..."):
                # å°†ä¸Šä¼ çš„æ–‡ä»¶ä¿å­˜åˆ°æœ¬åœ°
                save_path = "./knowledge_base"
                os.makedirs(save_path, exist_ok=True)
                for file in uploaded_files:
                    with open(os.path.join(save_path, file.name), "wb") as f:
                        f.write(file.getbuffer())
                
                # æ„å»ºå‘é‡æ•°æ®åº“
                load_and_embed_knowledge_base()
                st.success("çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
        else:
            st.warning("è¯·å…ˆä¸Šä¼ æ–‡ä»¶ã€‚")

# ---- å¯¹è¯çŠ¶æ€ç®¡ç† ----
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [
        AIMessage(content="æ‚¨å¥½ï¼æˆ‘æ˜¯å¯èˆªAIï¼Œæ‚¨çš„ä¸“å±ç•™å­¦è§„åˆ’å¸ˆã€‚æ— è®ºæ‚¨æœ‰ä»€ä¹ˆå…³äºé€‰æ ¡ã€æ–‡ä¹¦æˆ–ç”³è¯·ç­–ç•¥çš„é—®é¢˜ï¼Œéƒ½å¯ä»¥é—®æˆ‘ã€‚æ‚¨å¯ä»¥å…ˆåœ¨å·¦ä¾§ä¸Šä¼ æ‚¨çš„èƒŒæ™¯èµ„æ–™æˆ–æ¡ˆä¾‹åº“ã€‚")
    ]

# ---- å¯¹è¯ç•Œé¢ ----
for message in st.session_state.chat_history:
    with st.chat_message(message.type):
        st.write(message.content)

# ---- ç”¨æˆ·è¾“å…¥ ----
user_prompt = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...")
if user_prompt:
    st.session_state.chat_history.append(HumanMessage(content=user_prompt))
    with st.chat_message("human"):
        st.write(user_prompt)

    with st.chat_message("ai"):
        with st.spinner("å¯èˆªAIæ­£åœ¨æ€è€ƒ..."):
            # ä½¿ç”¨æµå¼è¾“å‡ºæ¥å®æ—¶æ˜¾ç¤ºAgentçš„æ€è€ƒè¿‡ç¨‹
            response_container = st.empty()
            full_response = ""
            
            # è°ƒç”¨ LangGraph
            events = agent_graph.stream({
                "input": user_prompt,
                "chat_history": st.session_state.chat_history,
                "intermediate_steps": []
            })
            
            for event in events:
                # æ‰“å°æ‰€æœ‰äº‹ä»¶ï¼Œæ–¹ä¾¿è°ƒè¯•
                # print(event) 
                
                if "agent" in event:
                    outcome = event["agent"].get("agent_outcome")
                    if outcome:
                        # å¦‚æœæ˜¯æœ€ç»ˆç­”æ¡ˆ
                        if not outcome.tool_calls:
                            full_response = outcome.return_values['output']
                            response_container.markdown(full_response)
                        else: # å¦‚æœæ˜¯å·¥å…·è°ƒç”¨
                            for tool_call in outcome.tool_calls:
                                tool_name = tool_call['name']
                                tool_args = tool_call['args']
                                full_response += f"æ­£åœ¨è°ƒç”¨å·¥å…·: `{tool_name}`\nå‚æ•°: `{tool_args}`\n\n"
                                response_container.markdown(full_response)
            
    st.session_state.chat_history.append(AIMessage(content=full_response))

```

-----

### **ç¬¬äº”æ­¥ï¼šè¿è¡Œä½ çš„é«˜çº§Agent**

1.  åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ª `.env` æ–‡ä»¶å¹¶å¡«å…¥ä½ çš„APIå¯†é’¥ã€‚
2.  åˆ›å»ºä¸€ä¸ª `knowledge_base` æ–‡ä»¶å¤¹ã€‚
3.  åœ¨ç»ˆç«¯ä¸­è¿è¡ŒStreamlitåº”ç”¨ï¼š
    ```bash
    streamlit run app.py
    ```
4.  æµè§ˆå™¨ä¼šè‡ªåŠ¨æ‰“å¼€ä¸€ä¸ªWebç•Œé¢ã€‚
      * **ç¬¬ä¸€æ­¥ï¼š** åœ¨å·¦ä¾§è¾¹æ ä¸Šä¼ ä½ çš„PDF/DOCæ–‡ä»¶ï¼ˆå¦‚ï¼šåæ ¡ç”³è¯·æˆåŠŸæ¡ˆä¾‹ã€å„é¡¹ç›®ä»‹ç»ã€æ–‡ä¹¦å†™ä½œæŒ‡å—ç­‰ï¼‰ã€‚
      * **ç¬¬äºŒæ­¥ï¼š** ç‚¹å‡»â€œæ„å»º/æ›´æ–°çŸ¥è¯†åº“â€æŒ‰é’®ï¼Œç­‰å¾…å¤„ç†å®Œæˆã€‚
      * **ç¬¬ä¸‰æ­¥ï¼š** åœ¨ä¸»å¯¹è¯æ¡†ä¸­å¼€å§‹ä¸â€œå¯èˆªAIâ€å¯¹è¯ã€‚

**æµ‹è¯•é—®é¢˜ç¤ºä¾‹ï¼š**

  * â€œæ ¹æ®çŸ¥è¯†åº“é‡Œçš„æˆåŠŸæ¡ˆä¾‹ï¼Œç”³è¯·CMUçš„CSé¡¹ç›®ï¼Œç®€å†ä¸Šåº”è¯¥çªå‡ºå“ªäº›æ–¹é¢ï¼Ÿâ€ (ä¼šè°ƒç”¨çŸ¥è¯†åº“)
  * â€œæœ€è¿‘çš„US Newsè®¡ç®—æœºç§‘å­¦ä¸“ä¸šæ’åæ˜¯æ€æ ·çš„ï¼Ÿâ€ (ä¼šè°ƒç”¨ç½‘ç»œæœç´¢)
  * â€œæˆ‘ä¸Šä¸€æ¡é—®é¢˜é—®çš„æ˜¯ä»€ä¹ˆï¼Ÿâ€ (æµ‹è¯•çŸ­æœŸè®°å¿†)