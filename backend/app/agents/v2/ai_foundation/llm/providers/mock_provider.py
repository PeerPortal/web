"""
æ¨¡æ‹Ÿ LLM æä¾›å•†å®ç°
ç”¨äºæµ‹è¯•å’Œå¼€å‘ç¯å¢ƒçš„æ¨¡æ‹Ÿå“åº”
"""
import time
import asyncio
import random
from typing import AsyncGenerator, List, Dict, Any
import uuid

from .base_provider import BaseLLMProvider, BaseEmbeddingProvider, LLMResponse, StreamChunk


class MockProvider(BaseLLMProvider):
    """æ¨¡æ‹Ÿ LLM æä¾›å•†"""
    
    def __init__(self, api_key: str = "mock", **kwargs):
        super().__init__(api_key, **kwargs)
        self.response_delay = kwargs.get('response_delay', 1.0)  # æ¨¡æ‹Ÿå“åº”å»¶è¿Ÿ
    
    async def chat(
        self, 
        messages: List[Dict[str, str]], 
        model: str,
        **kwargs
    ) -> LLMResponse:
        """æ¨¡æ‹ŸèŠå¤©å¯¹è¯"""
        start_time = time.time()
        
        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await asyncio.sleep(self.response_delay)
        
        # æ ¹æ®è¾“å…¥ç”Ÿæˆæ¨¡æ‹Ÿå“åº”
        last_message = messages[-1].get('content', '') if messages else ''
        
        # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„å“åº”
        if 'ä½ å¥½' in last_message or 'hello' in last_message.lower():
            content = "ä½ å¥½ï¼æˆ‘æ˜¯PeerPortalçš„AIç•™å­¦è§„åˆ’å¸ˆã€‚æˆ‘å¯ä»¥å¸®åŠ©æ‚¨åˆ¶å®šä¸ªæ€§åŒ–çš„ç•™å­¦ç”³è¯·ç­–ç•¥ï¼ŒåŒ…æ‹¬é€‰æ ¡å»ºè®®ã€æ–‡ä¹¦æ¶¦è‰²ã€é¢è¯•æŒ‡å¯¼ç­‰ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨çš„å…·ä½“éœ€æ±‚ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›ä¸“ä¸šçš„å»ºè®®ã€‚"
        elif 'åŠŸèƒ½' in last_message or 'feature' in last_message.lower():
            content = """æˆ‘çš„ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š

1. ğŸ¯ **ç•™å­¦è§„åˆ’**: æ ¹æ®æ‚¨çš„èƒŒæ™¯å’Œç›®æ ‡ï¼Œåˆ¶å®šä¸ªæ€§åŒ–çš„ç”³è¯·ç­–ç•¥
2. âœï¸ **æ–‡ä¹¦æ¶¦è‰²**: å¸®åŠ©ä¼˜åŒ–ä¸ªäººé™ˆè¿°ã€æ¨èä¿¡ç­‰ç”³è¯·ææ–™
3. ğŸ­ **é¢è¯•æŒ‡å¯¼**: æä¾›æ¨¡æ‹Ÿé¢è¯•å’ŒæŠ€å·§æŒ‡å¯¼
4. ğŸ« **é€‰æ ¡å»ºè®®**: åŸºäºæ‚¨çš„æ¡ä»¶æ¨èåˆé€‚çš„å­¦æ ¡å’Œä¸“ä¸š
5. ğŸ“Š **ç”³è¯·è§„åˆ’**: åˆ¶å®šè¯¦ç»†çš„æ—¶é—´è§„åˆ’å’Œå‡†å¤‡æ¸…å•

æœ‰ä»€ä¹ˆå…·ä½“é—®é¢˜æˆ‘å¯ä»¥å¸®æ‚¨è§£ç­”å—ï¼Ÿ"""
        elif 'ç”³è¯·' in last_message or 'apply' in last_message.lower():
            content = "å…³äºç•™å­¦ç”³è¯·ï¼Œæˆ‘éœ€è¦äº†è§£ä¸€äº›åŸºæœ¬ä¿¡æ¯æ¥ä¸ºæ‚¨æä¾›æ›´ç²¾å‡†çš„å»ºè®®ï¼š\n\n1. æ‚¨çš„ç›®æ ‡å›½å®¶å’Œåœ°åŒº\n2. å¸Œæœ›ç”³è¯·çš„ä¸“ä¸šé¢†åŸŸ\n3. æ‚¨çš„å­¦æœ¯èƒŒæ™¯å’Œæˆç»©\n4. è¯­è¨€è€ƒè¯•æˆç»©ï¼ˆæ‰˜ç¦/é›…æ€ç­‰ï¼‰\n5. é¢„æœŸçš„ç”³è¯·æ—¶é—´\n\nè¯·åˆ†äº«è¿™äº›ä¿¡æ¯ï¼Œæˆ‘ä¼šä¸ºæ‚¨åˆ¶å®šè¯¦ç»†çš„ç”³è¯·ç­–ç•¥ã€‚"
        else:
            content = f"æˆ‘ç†è§£æ‚¨æåˆ°çš„'{last_message[:50]}...'ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚ä½œä¸ºAIç•™å­¦è§„åˆ’å¸ˆï¼Œæˆ‘å»ºè®®æˆ‘ä»¬å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢æ¥åˆ†æï¼š\n\n1. æ˜ç¡®æ‚¨çš„å…·ä½“ç›®æ ‡å’Œéœ€æ±‚\n2. è¯„ä¼°å½“å‰çš„å‡†å¤‡æƒ…å†µ\n3. åˆ¶å®šå¯è¡Œçš„å®æ–½è®¡åˆ’\n\næ‚¨å¸Œæœ›æˆ‘é‡ç‚¹å…³æ³¨å“ªä¸ªæ–¹é¢å‘¢ï¼Ÿ"
        
        return LLMResponse(
            content=content,
            model=model,
            usage={
                "prompt_tokens": len(str(messages)),
                "completion_tokens": len(content),
                "total_tokens": len(str(messages)) + len(content)
            },
            finish_reason="stop",
            response_time=time.time() - start_time
        )
    
    async def stream_chat(
        self,
        messages: List[Dict[str, str]],
        model: str,
        **kwargs
    ) -> AsyncGenerator[StreamChunk, None]:
        """æ¨¡æ‹Ÿæµå¼èŠå¤©å¯¹è¯"""
        # å…ˆç”Ÿæˆå®Œæ•´å“åº”
        response = await self.chat(messages, model, **kwargs)
        content = response.content
        
        # æ¨¡æ‹Ÿé€å­—ç¬¦æµå¼è¾“å‡º
        for i, char in enumerate(content):
            await asyncio.sleep(0.02)  # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
            yield StreamChunk(
                content=char,
                is_complete=i == len(content) - 1,
                model=model,
                chunk_id=str(uuid.uuid4())
            )
    
    async def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return [
            "mock-gpt-4o-mini",
            "mock-gpt-3.5-turbo",
            "mock-gpt-4",
            "mock-claude-3"
        ]
    
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        # æ¨¡æ‹Ÿæ£€æŸ¥
        await asyncio.sleep(0.1)
        return True


class MockEmbeddingProvider(BaseEmbeddingProvider):
    """æ¨¡æ‹ŸåµŒå…¥æä¾›å•†"""
    
    def __init__(self, api_key: str = "mock", **kwargs):
        super().__init__(api_key, **kwargs)
        self.embedding_dim = kwargs.get('embedding_dim', 1536)
    
    async def embed_texts(
        self,
        texts: List[str],
        model: str,
        **kwargs
    ) -> List[List[float]]:
        """æ¨¡æ‹Ÿæ–‡æœ¬åµŒå…¥"""
        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await asyncio.sleep(0.1 * len(texts))
        
        embeddings = []
        for text in texts:
            # åŸºäºæ–‡æœ¬å†…å®¹ç”Ÿæˆä¼ªéšæœºä½†ä¸€è‡´çš„åµŒå…¥
            random.seed(hash(text) % (2**32))
            embedding = [random.gauss(0, 1) for _ in range(self.embedding_dim)]
            
            # å½’ä¸€åŒ–å‘é‡
            norm = sum(x**2 for x in embedding) ** 0.5
            if norm > 0:
                embedding = [x / norm for x in embedding]
            
            embeddings.append(embedding)
        
        return embeddings
    
    async def embed_query(
        self,
        query: str,
        model: str,
        **kwargs
    ) -> List[float]:
        """æ¨¡æ‹ŸæŸ¥è¯¢åµŒå…¥"""
        embeddings = await self.embed_texts([query], model, **kwargs)
        return embeddings[0] if embeddings else [0.0] * self.embedding_dim
    
    async def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨åµŒå…¥æ¨¡å‹åˆ—è¡¨"""
        return [
            "mock-text-embedding-ada-002",
            "mock-text-embedding-3-small",
            "mock-text-embedding-3-large"
        ]
    
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        await asyncio.sleep(0.1)
        return True 