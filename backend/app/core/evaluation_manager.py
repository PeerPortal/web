"""
LangSmith è¯„ä¼°æ•°æ®é›†ç®¡ç†å·¥å…·
ç”¨äºåˆ›å»ºã€ç®¡ç†å’Œä½¿ç”¨è¯„ä¼°æ•°æ®é›†æ¥æµ‹è¯•AIç•™å­¦è§„åˆ’å¸ˆçš„æ€§èƒ½
"""
import json
import asyncio
from typing import Dict, List, Any, Optional
from pathlib import Path

from app.core.langsmith_config import study_abroad_evaluator, is_langsmith_enabled
from app.agents.langgraph.agent_graph import AdvancedPlannerAgent


class StudyAbroadDatasetManager:
    """ç•™å­¦è§„åˆ’å¸ˆè¯„ä¼°æ•°æ®é›†ç®¡ç†å™¨"""
    
    def __init__(self):
        self.evaluator = study_abroad_evaluator
        self.agent = AdvancedPlannerAgent()
        self.datasets_dir = Path("evaluation_datasets")
        self.datasets_dir.mkdir(exist_ok=True)
    
    def create_standard_datasets(self):
        """åˆ›å»ºæ ‡å‡†è¯„ä¼°æ•°æ®é›†"""
        datasets = {
            "åŸºç¡€å’¨è¯¢é—®é¢˜": self._get_basic_consultation_examples(),
            "å­¦æ ¡æ¨èåœºæ™¯": self._get_school_recommendation_examples(),
            "ç”³è¯·è§„åˆ’åœºæ™¯": self._get_application_planning_examples(),
            "æ–‡ä¹¦æŒ‡å¯¼åœºæ™¯": self._get_essay_guidance_examples(),
            "æ—¶é—´è§„åˆ’åœºæ™¯": self._get_timeline_planning_examples()
        }
        
        for dataset_name, examples in datasets.items():
            self._create_dataset_with_examples(dataset_name, examples)
    
    def _create_dataset_with_examples(self, dataset_name: str, examples: List[Dict]):
        """åˆ›å»ºæ•°æ®é›†å¹¶æ·»åŠ ç¤ºä¾‹"""
        print(f"ğŸ“Š åˆ›å»ºè¯„ä¼°æ•°æ®é›†: {dataset_name}")
        
        # åˆ›å»ºæœ¬åœ°JSONæ–‡ä»¶
        dataset_file = self.datasets_dir / f"{dataset_name}.json"
        with open(dataset_file, 'w', encoding='utf-8') as f:
            json.dump(examples, f, ensure_ascii=False, indent=2)
        
        if is_langsmith_enabled():
            # åˆ›å»ºLangSmithæ•°æ®é›†
            dataset_id = self.evaluator.create_evaluation_dataset(
                dataset_name=dataset_name,
                description=f"AIç•™å­¦è§„åˆ’å¸ˆè¯„ä¼°æ•°æ®é›† - {dataset_name}"
            )
            
            if dataset_id:
                # æ·»åŠ ç¤ºä¾‹åˆ°LangSmith
                for example in examples:
                    self.evaluator.add_evaluation_example(
                        dataset_name=dataset_name,
                        input_data=example["input"],
                        expected_output=example["expected_output"],
                        metadata=example.get("metadata", {})
                    )
                print(f"âœ… LangSmithæ•°æ®é›†åˆ›å»ºæˆåŠŸ: {dataset_name}")
        else:
            print(f"ğŸ“ æœ¬åœ°æ•°æ®é›†æ–‡ä»¶å·²ä¿å­˜: {dataset_file}")
    
    def _get_basic_consultation_examples(self) -> List[Dict]:
        """åŸºç¡€å’¨è¯¢é—®é¢˜ç¤ºä¾‹"""
        return [
            {
                "input": {
                    "input": "æˆ‘æƒ³ç”³è¯·ç¾å›½çš„è®¡ç®—æœºç§‘å­¦ç¡•å£«ï¼Œéœ€è¦å‡†å¤‡ä»€ä¹ˆï¼Ÿ",
                    "user_id": "test_user_1"
                },
                "expected_output": {
                    "contains_keywords": ["GRE", "TOEFL", "GPA", "æ¨èä¿¡", "ä¸ªäººé™ˆè¿°", "ç ”ç©¶ç»å†"],
                    "response_type": "comprehensive_guidance",
                    "tone": "professional_friendly"
                },
                "metadata": {
                    "category": "åŸºç¡€å’¨è¯¢",
                    "difficulty": "easy",
                    "expected_tools": ["knowledge_base_search", "find_mentors_tool"]
                }
            },
            {
                "input": {
                    "input": "è‹±å›½å’Œç¾å›½çš„ç ”ç©¶ç”Ÿç”³è¯·æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
                    "user_id": "test_user_2"
                },
                "expected_output": {
                    "contains_keywords": ["ç”³è¯·æ—¶é—´", "å­¦åˆ¶", "è´¹ç”¨", "ç­¾è¯", "å½•å–è¦æ±‚"],
                    "response_type": "comparison_analysis",
                    "tone": "educational"
                },
                "metadata": {
                    "category": "å›½å®¶å¯¹æ¯”",
                    "difficulty": "medium",
                    "expected_tools": ["knowledge_base_search", "web_search"]
                }
            }
        ]
    
    def _get_school_recommendation_examples(self) -> List[Dict]:
        """å­¦æ ¡æ¨èåœºæ™¯ç¤ºä¾‹"""
        return [
            {
                "input": {
                    "input": "æˆ‘çš„GPAæ˜¯3.5ï¼ŒGRE320ï¼Œæƒ³ç”³è¯·ç¾å›½TOP50çš„è®¡ç®—æœºç§‘å­¦ç¡•å£«ï¼Œæœ‰ä»€ä¹ˆæ¨èï¼Ÿ",
                    "user_id": "test_user_3"
                },
                "expected_output": {
                    "contains_keywords": ["åŒ¹é…", "ä¿åº•", "å†²åˆº", "å…·ä½“å­¦æ ¡åç§°"],
                    "response_type": "personalized_recommendation",
                    "tone": "analytical"
                },
                "metadata": {
                    "category": "å­¦æ ¡æ¨è",
                    "difficulty": "medium",
                    "user_profile": {"gpa": 3.5, "gre": 320, "major": "cs"},
                    "expected_tools": ["web_search", "find_mentors_tool"]
                }
            }
        ]
    
    def _get_application_planning_examples(self) -> List[Dict]:
        """ç”³è¯·è§„åˆ’åœºæ™¯ç¤ºä¾‹"""
        return [
            {
                "input": {
                    "input": "æˆ‘ç°åœ¨å¤§ä¸‰ä¸‹å­¦æœŸï¼Œæƒ³æ˜å¹´ç§‹å­£å…¥å­¦ï¼Œåº”è¯¥æ€ä¹ˆè§„åˆ’ç”³è¯·æ—¶é—´ï¼Ÿ",
                    "user_id": "test_user_4"
                },
                "expected_output": {
                    "contains_keywords": ["æ—¶é—´è½´", "deadlines", "æ ‡å‡†åŒ–è€ƒè¯•", "æ–‡ä¹¦", "æ¨èä¿¡"],
                    "response_type": "timeline_planning",
                    "tone": "actionable"
                },
                "metadata": {
                    "category": "æ—¶é—´è§„åˆ’",
                    "difficulty": "medium",
                    "timeline": "junior_spring_to_senior_fall",
                    "expected_tools": ["knowledge_base_search"]
                }
            }
        ]
    
    def _get_essay_guidance_examples(self) -> List[Dict]:
        """æ–‡ä¹¦æŒ‡å¯¼åœºæ™¯ç¤ºä¾‹"""
        return [
            {
                "input": {
                    "input": "è®¡ç®—æœºç§‘å­¦çš„ä¸ªäººé™ˆè¿°åº”è¯¥åŒ…å«å“ªäº›å†…å®¹ï¼Ÿæœ‰ä»€ä¹ˆå†™ä½œæŠ€å·§ï¼Ÿ",
                    "user_id": "test_user_5"
                },
                "expected_output": {
                    "contains_keywords": ["ç ”ç©¶å…´è¶£", "é¡¹ç›®ç»å†", "èŒä¸šç›®æ ‡", "ç»“æ„", "æŠ€å·§"],
                    "response_type": "writing_guidance",
                    "tone": "instructional"
                },
                "metadata": {
                    "category": "æ–‡ä¹¦æŒ‡å¯¼",
                    "difficulty": "medium",
                    "document_type": "personal_statement",
                    "expected_tools": ["knowledge_base_search", "find_services_tool"]
                }
            }
        ]
    
    def _get_timeline_planning_examples(self) -> List[Dict]:
        """æ—¶é—´è§„åˆ’åœºæ™¯ç¤ºä¾‹"""
        return [
            {
                "input": {
                    "input": "2025å¹´ç§‹å­£å…¥å­¦çš„ç”³è¯·ï¼Œç°åœ¨12æœˆä»½è¿˜æ¥å¾—åŠå—ï¼Ÿ",
                    "user_id": "test_user_6"
                },
                "expected_output": {
                    "contains_keywords": ["spring intake", "deadline", "rush", "å»ºè®®"],
                    "response_type": "urgent_planning",
                    "tone": "realistic_supportive"
                },
                "metadata": {
                    "category": "ç´§æ€¥è§„åˆ’",
                    "difficulty": "hard",
                    "timing": "late_application_cycle",
                    "expected_tools": ["web_search", "knowledge_base_search"]
                }
            }
        ]
    
    async def run_evaluation_on_dataset(self, dataset_name: str) -> Dict[str, Any]:
        """åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šè¿è¡Œè¯„ä¼°"""
        dataset_file = self.datasets_dir / f"{dataset_name}.json"
        
        if not dataset_file.exists():
            print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_file}")
            return {}
        
        with open(dataset_file, 'r', encoding='utf-8') as f:
            examples = json.load(f)
        
        print(f"ğŸ§ª å¼€å§‹åœ¨æ•°æ®é›† '{dataset_name}' ä¸Šè¿è¡Œè¯„ä¼°...")
        results = []
        
        for i, example in enumerate(examples):
            print(f"  æ­£åœ¨è¯„ä¼°ç¤ºä¾‹ {i+1}/{len(examples)}...")
            
            try:
                # è¿è¡ŒAgent
                agent_response = await self.agent.ainvoke(example["input"])
                
                # ç®€å•çš„è¯„ä¼°é€»è¾‘
                evaluation_result = self._evaluate_response(
                    agent_response,
                    example["expected_output"],
                    example.get("metadata", {})
                )
                
                results.append({
                    "example_id": i,
                    "input": example["input"],
                    "agent_output": agent_response["output"],
                    "expected": example["expected_output"],
                    "evaluation": evaluation_result,
                    "execution_time": agent_response.get("metadata", {}).get("execution_time", 0)
                })
                
            except Exception as e:
                print(f"    âŒ ç¤ºä¾‹ {i+1} æ‰§è¡Œå¤±è´¥: {str(e)}")
                results.append({
                    "example_id": i,
                    "error": str(e)
                })
        
        # è®¡ç®—æ€»ä½“è¯„ä¼°ç»“æœ
        evaluation_summary = self._calculate_evaluation_summary(results)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        result_file = self.datasets_dir / f"{dataset_name}_evaluation_results.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump({
                "dataset_name": dataset_name,
                "evaluation_summary": evaluation_summary,
                "detailed_results": results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜: {result_file}")
        return evaluation_summary
    
    def _evaluate_response(
        self, 
        agent_response: Dict, 
        expected_output: Dict, 
        metadata: Dict
    ) -> Dict[str, Any]:
        """è¯„ä¼°Agentå“åº”è´¨é‡"""
        output = agent_response.get("output", "")
        
        # æ£€æŸ¥å…³é”®è¯è¦†ç›–ç‡
        expected_keywords = expected_output.get("contains_keywords", [])
        found_keywords = [kw for kw in expected_keywords if kw.lower() in output.lower()]
        keyword_coverage = len(found_keywords) / len(expected_keywords) if expected_keywords else 1.0
        
        # æ£€æŸ¥å“åº”é•¿åº¦ï¼ˆå¤ªçŸ­å¯èƒ½ä¸å¤Ÿè¯¦ç»†ï¼‰
        response_length_score = min(1.0, len(output) / 200) if output else 0.0
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é”™è¯¯
        has_error = "æŠ±æ­‰" in output or "é”™è¯¯" in output or "å¤±è´¥" in output
        error_penalty = 0.3 if has_error else 0.0
        
        # ç»¼åˆè¯„åˆ†
        overall_score = max(0.0, keyword_coverage * 0.6 + response_length_score * 0.4 - error_penalty)
        
        return {
            "keyword_coverage": keyword_coverage,
            "found_keywords": found_keywords,
            "response_length": len(output),
            "response_length_score": response_length_score,
            "has_error": has_error,
            "overall_score": overall_score,
            "grade": self._score_to_grade(overall_score)
        }
    
    def _score_to_grade(self, score: float) -> str:
        """å°†æ•°å€¼è¯„åˆ†è½¬æ¢ä¸ºç­‰çº§"""
        if score >= 0.9:
            return "A"
        elif score >= 0.8:
            return "B"
        elif score >= 0.7:
            return "C"
        elif score >= 0.6:
            return "D"
        else:
            return "F"
    
    def _calculate_evaluation_summary(self, results: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—è¯„ä¼°æ±‡æ€»ç»“æœ"""
        successful_results = [r for r in results if "error" not in r]
        failed_count = len(results) - len(successful_results)
        
        if not successful_results:
            return {
                "total_examples": len(results),
                "successful_examples": 0,
                "failed_examples": failed_count,
                "success_rate": 0.0,
                "average_score": 0.0,
                "grade_distribution": {}
            }
        
        scores = [r["evaluation"]["overall_score"] for r in successful_results]
        grades = [r["evaluation"]["grade"] for r in successful_results]
        
        grade_distribution = {}
        for grade in ["A", "B", "C", "D", "F"]:
            grade_distribution[grade] = grades.count(grade)
        
        return {
            "total_examples": len(results),
            "successful_examples": len(successful_results),
            "failed_examples": failed_count,
            "success_rate": len(successful_results) / len(results),
            "average_score": sum(scores) / len(scores),
            "average_keyword_coverage": sum(r["evaluation"]["keyword_coverage"] for r in successful_results) / len(successful_results),
            "grade_distribution": grade_distribution,
            "execution_stats": {
                "avg_execution_time": sum(r.get("execution_time", 0) for r in successful_results) / len(successful_results),
                "max_execution_time": max(r.get("execution_time", 0) for r in successful_results),
                "min_execution_time": min(r.get("execution_time", 0) for r in successful_results)
            }
        }
    
    def get_standard_datasets(self) -> Dict[str, Dict]:
        """è·å–æ ‡å‡†è¯„ä¼°æ•°æ®é›†"""
        datasets = {}
        
        # åŸºç¡€å’¨è¯¢æ•°æ®é›†
        datasets["basic_consultation"] = {
            "name": "åŸºç¡€ç•™å­¦å’¨è¯¢",
            "description": "æµ‹è¯•Agentå¯¹åŸºç¡€ç•™å­¦é—®é¢˜çš„å›ç­”è´¨é‡",
            "examples": [
                {
                    "input": "æˆ‘æƒ³ç”³è¯·ç¾å›½çš„è®¡ç®—æœºç§‘å­¦ç¡•å£«ï¼Œéœ€è¦ä»€ä¹ˆæ¡ä»¶ï¼Ÿ",
                    "expected_topics": ["GPAè¦æ±‚", "è¯­è¨€æˆç»©", "GRE", "å·¥ä½œç»éªŒ", "æ¨èä¿¡"],
                    "evaluation_criteria": ["å‡†ç¡®æ€§", "å®Œæ•´æ€§", "å®ç”¨æ€§"]
                },
                {
                    "input": "è‹±å›½å’Œç¾å›½çš„ç•™å­¦è´¹ç”¨å¤§æ¦‚æ˜¯å¤šå°‘ï¼Ÿ",
                    "expected_topics": ["å­¦è´¹", "ç”Ÿæ´»è´¹", "å¥–å­¦é‡‘", "æ‰“å·¥æ”¿ç­–"],
                    "evaluation_criteria": ["å‡†ç¡®æ€§", "æ—¶æ•ˆæ€§", "å¯¹æ¯”åˆ†æ"]
                },
                {
                    "input": "æˆ‘çš„èƒŒæ™¯é€‚åˆç”³è¯·å“ªäº›å­¦æ ¡ï¼Ÿ",
                    "expected_topics": ["ä¸ªäººè¯„ä¼°", "å­¦æ ¡æ¨è", "ç”³è¯·ç­–ç•¥"],
                    "evaluation_criteria": ["ä¸ªæ€§åŒ–", "å‡†ç¡®æ€§", "å¯æ“ä½œæ€§"]
                }
            ]
        }
        
        # ä¸“ä¸šè§„åˆ’æ•°æ®é›†
        datasets["career_planning"] = {
            "name": "ä¸“ä¸šä¸èŒä¸šè§„åˆ’", 
            "description": "æµ‹è¯•Agentå¯¹ä¸“ä¸šé€‰æ‹©å’ŒèŒä¸šè§„åˆ’çš„å»ºè®®è´¨é‡",
            "examples": [
                {
                    "input": "æˆ‘å­¦çš„æ˜¯é‡‘èï¼Œæƒ³è½¬åˆ°æ•°æ®ç§‘å­¦ï¼Œæœ‰ä»€ä¹ˆå»ºè®®ï¼Ÿ",
                    "expected_topics": ["è½¬ä¸“ä¸šå‡†å¤‡", "å…ˆä¿®è¯¾ç¨‹", "é¡¹ç›®ç»éªŒ", "æŠ€èƒ½æå‡"],
                    "evaluation_criteria": ["è½¬æ¢æ€§", "å®ç”¨æ€§", "å¯è¡Œæ€§"]
                },
                {
                    "input": "äººå·¥æ™ºèƒ½ä¸“ä¸šçš„å°±ä¸šå‰æ™¯å¦‚ä½•ï¼Ÿ",
                    "expected_topics": ["è¡Œä¸šè¶‹åŠ¿", "å°±ä¸šæœºä¼š", "è–ªèµ„æ°´å¹³", "æŠ€èƒ½è¦æ±‚"],
                    "evaluation_criteria": ["å‰ç»æ€§", "å‡†ç¡®æ€§", "å…¨é¢æ€§"]
                }
            ]
        }
        
        # ç”³è¯·æµç¨‹æ•°æ®é›†
        datasets["application_process"] = {
            "name": "ç”³è¯·æµç¨‹æŒ‡å¯¼",
            "description": "æµ‹è¯•Agentå¯¹ç”³è¯·æµç¨‹çš„æŒ‡å¯¼è´¨é‡",
            "examples": [
                {
                    "input": "ç”³è¯·ç ”ç©¶ç”Ÿçš„æ—¶é—´çº¿æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ",
                    "expected_topics": ["ç”³è¯·æ—¶é—´è¡¨", "å‡†å¤‡é˜¶æ®µ", "æˆªæ­¢æ—¥æœŸ", "å…³é”®èŠ‚ç‚¹"],
                    "evaluation_criteria": ["æ—¶åºæ€§", "å®Œæ•´æ€§", "å®ç”¨æ€§"]
                },
                {
                    "input": "å¦‚ä½•å†™å¥½ä¸ªäººé™ˆè¿°ï¼Ÿ",
                    "expected_topics": ["ç»“æ„è¦æ±‚", "å†…å®¹è¦ç‚¹", "å†™ä½œæŠ€å·§", "å¸¸è§é”™è¯¯"],
                    "evaluation_criteria": ["æŒ‡å¯¼æ€§", "å¯æ“ä½œæ€§", "ä¸“ä¸šæ€§"]
                }
            ]
        }
        
        return datasets


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """è¿è¡Œè¯„ä¼°ç¤ºä¾‹"""
    dataset_manager = StudyAbroadDatasetManager()
    
    # åˆ›å»ºæ ‡å‡†æ•°æ®é›†
    print("ğŸ“Š åˆ›å»ºæ ‡å‡†è¯„ä¼°æ•°æ®é›†...")
    dataset_manager.create_standard_datasets()
    
    # è¿è¡Œè¯„ä¼°
    print("\nğŸ§ª è¿è¡ŒåŸºç¡€å’¨è¯¢é—®é¢˜è¯„ä¼°...")
    results = await dataset_manager.run_evaluation_on_dataset("åŸºç¡€å’¨è¯¢é—®é¢˜")
    
    print(f"\nğŸ“ˆ è¯„ä¼°ç»“æœæ±‡æ€»:")
    print(f"  æˆåŠŸç‡: {results.get('success_rate', 0):.1%}")
    print(f"  å¹³å‡å¾—åˆ†: {results.get('average_score', 0):.2f}")
    print(f"  ç­‰çº§åˆ†å¸ƒ: {results.get('grade_distribution', {})}")


if __name__ == "__main__":
    asyncio.run(main())
