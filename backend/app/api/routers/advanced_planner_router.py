"""
é«˜çº§AIç•™å­¦è§„åˆ’å¸ˆçš„APIè·¯ç”±
åŸºäºLangGraphå®ç°çš„å¢å¼ºç‰ˆæ™ºèƒ½å’¨è¯¢æœåŠ¡
é›†æˆLangSmithç›‘æ§å’Œè¯„ä¼°
"""
from fastapi import APIRouter, HTTPException, UploadFile, File
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import json
import time
import asyncio
from datetime import datetime

from app.agents.langgraph.agent_graph import get_advanced_agent
from app.agents.langgraph.knowledge_base import knowledge_manager
from app.core.langsmith_config import is_langsmith_enabled, study_abroad_tracer

router = APIRouter(prefix="/advanced-planner", tags=["é«˜çº§AIç•™å­¦è§„åˆ’å¸ˆ"])

class AdvancedPlannerRequest(BaseModel):
    """é«˜çº§AIè§„åˆ’å¸ˆè¯·æ±‚æ¨¡å‹"""
    input: str = Field(..., min_length=1, max_length=2000, description="ç”¨æˆ·çš„ç•™å­¦å’¨è¯¢é—®é¢˜")
    user_id: Optional[str] = Field(default="anonymous", description="ç”¨æˆ·IDï¼Œç”¨äºLangSmithè¿½è¸ª")
    session_id: Optional[str] = Field(default="default", description="ä¼šè¯IDï¼Œç”¨äºç»´æŒå¯¹è¯ä¸Šä¸‹æ–‡")
    chat_history: Optional[List[dict]] = Field(default=[], description="å¯¹è¯å†å²")
    stream: bool = Field(default=False, description="æ˜¯å¦å¯ç”¨æµå¼å“åº”")

class AdvancedPlannerResponse(BaseModel):
    """é«˜çº§AIè§„åˆ’å¸ˆå“åº”æ¨¡å‹"""
    output: str = Field(..., description="AIçš„å›ç­”å†…å®¹")
    session_id: str = Field(..., description="ä¼šè¯ID")
    timestamp: str = Field(..., description="å“åº”æ—¶é—´æˆ³")
    metadata: Optional[Dict[str, Any]] = Field(default={}, description="å“åº”å…ƒæ•°æ®ï¼ˆæ‰§è¡Œæ—¶é—´ã€å·¥å…·è°ƒç”¨ç­‰ï¼‰")
    langsmith_enabled: bool = Field(default=False, description="LangSmithæ˜¯å¦å¯ç”¨")

class KnowledgeBaseStatus(BaseModel):
    """çŸ¥è¯†åº“çŠ¶æ€æ¨¡å‹"""
    files_count: int = Field(..., description="æ–‡æ¡£æ•°é‡")
    vector_store_exists: bool = Field(..., description="å‘é‡åº“æ˜¯å¦å­˜åœ¨")
    files: List[str] = Field(..., description="æ–‡ä»¶åˆ—è¡¨")

def get_agent():
    """è·å–Agentå®ä¾‹"""
    try:
        return get_advanced_agent()
    except Exception as e:
        print(f"âŒ é«˜çº§Agentåˆå§‹åŒ–å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"AIæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {str(e)}")

@router.post("/invoke", 
             response_model=AdvancedPlannerResponse,
             summary="è°ƒç”¨é«˜çº§AIç•™å­¦è§„åˆ’å¸ˆ", 
             description="å‘é€é—®é¢˜ç»™é«˜çº§AIç•™å­¦è§„åˆ’å¸ˆï¼Œè·å¾—åŸºäºçŸ¥è¯†åº“å’Œå®æ—¶æœç´¢çš„ä¸“ä¸šå»ºè®®")
async def invoke_advanced_planner(request: AdvancedPlannerRequest):
    """
    è°ƒç”¨é«˜çº§AIç•™å­¦è§„åˆ’å¸ˆè¿›è¡Œå’¨è¯¢
    
    æ–°ç‰¹æ€§ï¼š
    - ğŸ§  é•¿æœŸè®°å¿†ï¼šè·¨ä¼šè¯è®°å¿†ç”¨æˆ·ä¿¡æ¯
    - ğŸ“š çŸ¥è¯†åº“æ£€ç´¢ï¼šä»ä¸Šä¼ æ–‡æ¡£ä¸­è·å–ä¸“ä¸šå»ºè®®  
    - ğŸ” æ™ºèƒ½æœç´¢ï¼šè‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„ä¿¡æ¯æº
    - âš¡ å·¥å…·èåˆï¼šæ•°æ®åº“æŸ¥è¯¢+ç½‘ç»œæœç´¢+çŸ¥è¯†åº“
    """
    try:
        agent = get_agent()
        
        if request.stream:
            # æµå¼å“åº”ï¼ˆæš‚æ—¶ä¿æŒåŸæœ‰é€»è¾‘ï¼Œæœªæ¥å¯æ‰©å±•LangSmithæµå¼è¿½è¸ªï¼‰
            return StreamingResponse(
                stream_generator(agent, request),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no"
                }
            )
        else:
            # éæµå¼å“åº”
            start_time = time.time()
            
            input_data = {
                "input": request.input,
                "session_id": request.session_id,
                "chat_history": request.chat_history or []
            }
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨LangSmithè¿½è¸ª
            langsmith_enabled = is_langsmith_enabled()
            
            if langsmith_enabled:
                # ä½¿ç”¨LangSmithè¿½è¸ªæ‰§è¡ŒAgent
                async with study_abroad_tracer.trace_agent_run(
                    agent_name="AIç•™å­¦è§„åˆ’å¸ˆ",
                    user_id=request.user_id,
                    session_id=request.session_id,
                    input_data=input_data
                ) as trace_context:
                    result = await agent.ainvoke(input_data)
                    
                    # æ„å»ºå…ƒæ•°æ®
                    metadata = {
                        "execution_time": time.time() - start_time,
                        "langsmith_run_id": trace_context.get("run_id") if trace_context else None,
                        "user_id": request.user_id,
                        "session_id": request.session_id
                    }
            else:
                # æ ‡å‡†æ‰§è¡Œï¼ˆæ— è¿½è¸ªï¼‰
                result = await agent.ainvoke(input_data)
                
                metadata = {
                    "execution_time": time.time() - start_time,
                    "user_id": request.user_id,
                    "session_id": request.session_id
                }
            
            return AdvancedPlannerResponse(
                output=result["output"],
                session_id=result["session_id"],
                timestamp=datetime.now().isoformat(),
                metadata=metadata,
                langsmith_enabled=langsmith_enabled
            )
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")

async def stream_generator(agent, request: AdvancedPlannerRequest):
    """æµå¼å“åº”ç”Ÿæˆå™¨"""
    try:
        input_data = {
            "input": request.input,
            "session_id": request.session_id,
            "chat_history": request.chat_history or []
        }
        
        # å‘é€å¼€å§‹ä¿¡å·
        yield f"data: {json.dumps({'type': 'start', 'message': 'å¼€å§‹å¤„ç†æ‚¨çš„é—®é¢˜...'}, ensure_ascii=False)}\n\n"
        
        # æµå¼å¤„ç†
        full_response = ""
        for event in agent.stream(input_data):
            if "agent" in event:
                outcome = event["agent"].get("agent_outcome")
                if outcome and hasattr(outcome, 'return_values'):
                    chunk = outcome.return_values.get('output', '')
                    if chunk:
                        full_response += chunk
                        yield f"data: {json.dumps({'type': 'chunk', 'chunk': chunk}, ensure_ascii=False)}\n\n"
            
            if "tools" in event:
                # å·¥å…·è°ƒç”¨ä¿¡æ¯
                tool_info = "æ­£åœ¨è°ƒç”¨å·¥å…·è·å–ä¿¡æ¯..."
                yield f"data: {json.dumps({'type': 'tool', 'message': tool_info}, ensure_ascii=False)}\n\n"
        
        # å‘é€å®Œæˆä¿¡å·
        yield f"data: {json.dumps({'type': 'end', 'full_response': full_response}, ensure_ascii=False)}\n\n"
        yield "data: [DONE]\n\n"
        
    except Exception as e:
        error_data = {
            "type": "error",
            "error": f"æµå¼å¤„ç†å‡ºé”™: {str(e)}"
        }
        yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"

@router.get("/health", summary="é«˜çº§AIæœåŠ¡å¥åº·æ£€æŸ¥")
async def health_check():
    """æ£€æŸ¥é«˜çº§AIç•™å­¦è§„åˆ’å¸ˆæœåŠ¡çŠ¶æ€"""
    try:
        agent = get_agent()
        kb_stats = knowledge_manager.get_knowledge_base_stats()
        
        return {
            "status": "healthy",
            "service": "é«˜çº§AIç•™å­¦è§„åˆ’å¸ˆ",
            "version": "2.0",
            "features": ["LangGraph", "çŸ¥è¯†åº“", "é•¿æœŸè®°å¿†", "å¤šå·¥å…·èåˆ"],
            "timestamp": datetime.now().isoformat(),
            "knowledge_base": {
                "files_count": kb_stats["files_count"],
                "vector_store_ready": kb_stats["vector_store_exists"]
            }
        }
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"é«˜çº§AIæœåŠ¡ä¸å¯ç”¨: {str(e)}")

@router.post("/upload-documents", summary="ä¸Šä¼ æ–‡æ¡£åˆ°çŸ¥è¯†åº“")
async def upload_documents(files: List[UploadFile] = File(...)):
    """
    ä¸Šä¼ PDFæ–‡æ¡£åˆ°çŸ¥è¯†åº“
    
    æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š
    - PDF: ç•™å­¦æ¡ˆä¾‹ã€å­¦æ ¡ä»‹ç»ã€ç”³è¯·æŒ‡å—ç­‰
    """
    try:
        uploaded_files = []
        
        for file in files:
            if not file.filename.lower().endswith('.pdf'):
                raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file.filename}")
            
            # ä¿å­˜æ–‡ä»¶
            file_content = await file.read()
            file_path = knowledge_manager.save_uploaded_file(file_content, file.filename)
            uploaded_files.append(file.filename)
        
        # é‡å»ºçŸ¥è¯†åº“
        vectorstore = knowledge_manager.load_and_embed_knowledge_base()
        
        return {
            "message": f"æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶",
            "files": uploaded_files,
            "knowledge_base_ready": vectorstore is not None,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")

@router.get("/knowledge-base/status", 
            response_model=KnowledgeBaseStatus,
            summary="è·å–çŸ¥è¯†åº“çŠ¶æ€")
async def get_knowledge_base_status():
    """è·å–çŸ¥è¯†åº“çš„çŠ¶æ€ä¿¡æ¯"""
    try:
        stats = knowledge_manager.get_knowledge_base_stats()
        return KnowledgeBaseStatus(**stats)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–çŸ¥è¯†åº“çŠ¶æ€å¤±è´¥: {str(e)}")

@router.delete("/knowledge-base", summary="æ¸…ç©ºçŸ¥è¯†åº“")
async def clear_knowledge_base():
    """æ¸…ç©ºçŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£å’Œå‘é‡æ•°æ®"""
    try:
        import shutil
        import os
        
        # åˆ é™¤çŸ¥è¯†åº“æ–‡ä»¶
        if os.path.exists("./knowledge_base"):
            shutil.rmtree("./knowledge_base")
        
        # åˆ é™¤å‘é‡åº“
        if os.path.exists("./vector_store"):
            shutil.rmtree("./vector_store")
        
        return {
            "message": "çŸ¥è¯†åº“å·²æ¸…ç©º",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}")

@router.get("/capabilities", summary="è·å–é«˜çº§AIèƒ½åŠ›è¯´æ˜")
async def get_capabilities():
    """è·å–é«˜çº§AIç•™å­¦è§„åˆ’å¸ˆçš„å®Œæ•´èƒ½åŠ›è¯´æ˜"""
    return {
        "service_name": "å¯èˆªAIç•™å­¦è§„åˆ’å¸ˆ 2.0",
        "core_features": {
            "intelligent_routing": "æ™ºèƒ½é€‰æ‹©æœ€åˆé€‚çš„ä¿¡æ¯æº",
            "knowledge_base": "ä»ä¸Šä¼ æ–‡æ¡£ä¸­å­¦ä¹ å¹¶æä¾›ä¸“ä¸šå»ºè®®",
            "long_term_memory": "è·¨ä¼šè¯è®°å¿†ç”¨æˆ·ä¿¡æ¯å’Œåå¥½",
            "real_time_search": "è·å–æœ€æ–°çš„ç•™å­¦èµ„è®¯å’Œå¤§å­¦ä¿¡æ¯",
            "platform_integration": "åŒ¹é…å¹³å°ä¸Šçš„å¼•è·¯äººå’ŒæœåŠ¡"
        },
        "capabilities": [
            "ğŸ§  æ™ºèƒ½å·¥å…·é€‰æ‹©ï¼šæ ¹æ®é—®é¢˜ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä½³ä¿¡æ¯æº",
            "ğŸ“š çŸ¥è¯†åº“å­¦ä¹ ï¼šä»PDFæ–‡æ¡£ä¸­å­¦ä¹ ä¸“ä¸šç•™å­¦çŸ¥è¯†",
            "ğŸ” å®æ—¶ä¿¡æ¯è·å–ï¼šæœç´¢æœ€æ–°çš„å¤§å­¦æ’åå’Œç”³è¯·è¦æ±‚",
            "ğŸ‘¥ ä¸ªæ€§åŒ–åŒ¹é…ï¼šæ¨èæœ€åˆé€‚çš„å¼•è·¯äººå’ŒæœåŠ¡",
            "ğŸ’­ ä¸Šä¸‹æ–‡è®°å¿†ï¼šç»´æŒå¯¹è¯è¿è´¯æ€§å’Œä¸ªæ€§åŒ–ä½“éªŒ",
            "âš¡ å¤šæ¨¡æ€å“åº”ï¼šæ”¯æŒæµå¼å’Œæ ‡å‡†å“åº”æ¨¡å¼"
        ],
        "supported_regions": ["ç¾å›½", "åŠ æ‹¿å¤§", "è‹±å›½", "æ¾³å¤§åˆ©äºš", "æ–°åŠ å¡", "é¦™æ¸¯", "å¾·å›½", "æ³•å›½"],
        "supported_degrees": ["æœ¬ç§‘", "ç¡•å£«", "åšå£«", "äº¤æ¢é¡¹ç›®"],
        "technical_stack": ["LangGraph", "OpenAI GPT", "ChromaDB", "Tavily Search", "FastAPI"],
        "response_modes": ["æµå¼å“åº”", "æ ‡å‡†å“åº”", "å·¥å…·è°ƒç”¨è¿½è¸ª"]
    }
