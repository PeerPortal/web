"""
çŸ¥è¯†åº“ç®¡ç†æ¨¡å—
å®ç°æ–‡æ¡£å¤„ç†ã€å‘é‡åŒ–å­˜å‚¨å’ŒRAGæ£€ç´¢åŠŸèƒ½
"""
import os
import shutil
from typing import List, Optional
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.tools import tool
from app.core.config import settings

VECTOR_STORE_PATH = "./vector_store"
KNOWLEDGE_BASE_PATH = "./knowledge_base"

class KnowledgeBaseManager:
    """çŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=settings.OPENAI_API_KEY,
            chunk_size=100  # å‡å°æ‰¹å¤„ç†å¤§å°ï¼Œé¿å…è¶…è¿‡tokené™åˆ¶
        )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,  # å‡å°chunkå¤§å°
            chunk_overlap=50  # å‡å°é‡å 
        )
    
    def save_uploaded_file(self, file_content: bytes, filename: str) -> str:
        """ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°çŸ¥è¯†åº“ç›®å½•"""
        os.makedirs(KNOWLEDGE_BASE_PATH, exist_ok=True)
        file_path = os.path.join(KNOWLEDGE_BASE_PATH, filename)
        
        with open(file_path, "wb") as f:
            f.write(file_content)
        
        return file_path
    
    def load_and_embed_knowledge_base(self) -> Optional[Chroma]:
        """åŠ è½½ã€åˆ†å‰²ã€åµŒå…¥å¹¶å­˜å‚¨çŸ¥è¯†åº“æ–‡æ¡£"""
        print("ğŸ” æ‰«æçŸ¥è¯†åº“æ–‡ä»¶...")
        
        # 1. æ£€æŸ¥çŸ¥è¯†åº“ç›®å½•
        if not os.path.exists(KNOWLEDGE_BASE_PATH) or not os.listdir(KNOWLEDGE_BASE_PATH):
            print("ğŸ“‚ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè·³è¿‡åµŒå…¥è¿‡ç¨‹ã€‚")
            return None
        
        # 2. åŠ è½½æ–‡æ¡£
        documents = []
        for file_name in os.listdir(KNOWLEDGE_BASE_PATH):
            file_path = os.path.join(KNOWLEDGE_BASE_PATH, file_name)
            if file_name.lower().endswith('.pdf'):
                loader = PyPDFLoader(file_path)
                docs = loader.load()
                documents.extend(docs)
            elif file_name.lower().endswith(('.txt', '.md')):
                # æ”¯æŒæ–‡æœ¬æ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    from langchain.schema import Document
                    doc = Document(page_content=content, metadata={'source': file_path})
                    documents.append(doc)
        
        if not documents:
            print("ğŸ“ æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ ¼å¼ï¼Œè·³è¿‡åµŒå…¥è¿‡ç¨‹ã€‚")
            return None
        
        print(f"ğŸ“š æ‰¾åˆ° {len(documents)} ä¸ªæ–‡æ¡£é¡µé¢")
        
        # 3. åˆ†å‰²æ–‡æ¡£
        splits = self.text_splitter.split_documents(documents)
        print(f"âœ‚ï¸ æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…± {len(splits)} ä¸ªæ–‡æ¡£å—")
        
        # å¦‚æœæ–‡æ¡£å—å¤ªå¤šï¼Œé™åˆ¶æ•°é‡é¿å…tokenè¶…é™
        if len(splits) > 500:
            print(f"âš ï¸ æ–‡æ¡£å—æ•°é‡({len(splits)})è¿‡å¤šï¼Œåªå¤„ç†å‰500ä¸ªé¿å…tokenè¶…é™")
            splits = splits[:500]
        
        # 4. åˆ›å»ºå¹¶æŒä¹…åŒ–å‘é‡æ•°æ®åº“
        print("ğŸ§  æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“...")
        
        # å¦‚æœå‘é‡åº“å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
        if os.path.exists(VECTOR_STORE_PATH):
            shutil.rmtree(VECTOR_STORE_PATH)
        
        try:
            # æ‰¹é‡å¤„ç†ï¼Œæ¯æ¬¡å¤„ç†50ä¸ªæ–‡æ¡£å—
            batch_size = 50
            vectorstore = None
            
            for i in range(0, len(splits), batch_size):
                batch = splits[i:i+batch_size]
                print(f"  ğŸ“¦ å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹ï¼Œå…± {len(batch)} ä¸ªæ–‡æ¡£å—")
                
                if vectorstore is None:
                    # ç¬¬ä¸€æ‰¹åˆ›å»ºå‘é‡åº“
                    vectorstore = Chroma.from_documents(
                        documents=batch,
                        embedding=self.embeddings,
                        persist_directory=VECTOR_STORE_PATH,
                        collection_name="knowledge_base"
                    )
                else:
                    # åç»­æ‰¹æ¬¡æ·»åŠ åˆ°ç°æœ‰å‘é‡åº“
                    vectorstore.add_documents(batch)
            
        except Exception as e:
            print(f"âš ï¸ ChromaDBåˆ›å»ºå¤±è´¥: {e}")
            # é™çº§å¤„ç†ï¼šåªä½¿ç”¨ç•™å­¦ç”³è¯·æˆåŠŸæ¡ˆä¾‹.txt
            txt_files = [doc for doc in documents if doc.metadata.get('source', '').endswith('.txt')]
            if txt_files:
                print("ğŸ”„ é™çº§å¤„ç†ï¼šåªä½¿ç”¨TXTæ–‡ä»¶é‡å»ºçŸ¥è¯†åº“")
                txt_splits = self.text_splitter.split_documents(txt_files)
                try:
                    os.makedirs(VECTOR_STORE_PATH, exist_ok=True)
                    vectorstore = Chroma.from_documents(
                        documents=txt_splits,
                        embedding=self.embeddings,
                        persist_directory=VECTOR_STORE_PATH,
                        collection_name="knowledge_base"
                    )
                except Exception as e2:
                    print(f"âŒ é™çº§å¤„ç†ä¹Ÿå¤±è´¥: {e2}")
                    return None
            else:
                return None
        
        print("âœ… å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆï¼")
        return vectorstore
    
    def get_retriever(self, k: int = 3):
        """è·å–ç°æœ‰çš„å‘é‡æ•°æ®åº“æ£€ç´¢å™¨"""
        if not os.path.exists(VECTOR_STORE_PATH):
            # å¦‚æœå‘é‡åº“ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»º
            vectorstore = self.load_and_embed_knowledge_base()
            if vectorstore is None:
                return None
        else:
            try:
                vectorstore = Chroma(
                    persist_directory=VECTOR_STORE_PATH,
                    embedding_function=self.embeddings,
                    collection_name="knowledge_base"
                )
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
                # å¦‚æœåŠ è½½å¤±è´¥ï¼Œå°è¯•é‡æ–°åˆ›å»º
                vectorstore = self.load_and_embed_knowledge_base()
                if vectorstore is None:
                    return None
        
        return vectorstore.as_retriever(search_kwargs={"k": k})
    
    def get_knowledge_base_stats(self) -> dict:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "files_count": 0,
            "vector_store_exists": os.path.exists(VECTOR_STORE_PATH),
            "files": []
        }
        
        if os.path.exists(KNOWLEDGE_BASE_PATH):
            files = os.listdir(KNOWLEDGE_BASE_PATH)
            stats["files_count"] = len(files)
            stats["files"] = files
        
        return stats

# åˆ›å»ºå…¨å±€å®ä¾‹
knowledge_manager = KnowledgeBaseManager()

@tool
def knowledge_base_retriever(query: str) -> List[str]:
    """
    ä»ç§æœ‰çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚
    å½“éœ€è¦å›ç­”å…³äºç•™å­¦ç”³è¯·ç­–ç•¥ã€ç‰¹å®šå­¦æ ¡çš„å†…éƒ¨ä¿¡æ¯ã€æ–‡ä¹¦å†™ä½œæŠ€å·§æˆ–è¿‡å¾€æˆåŠŸæ¡ˆä¾‹æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚
    è¿™ä¸ªå·¥å…·èƒ½ä»å¹³å°çš„ç§æœ‰çŸ¥è¯†åº“ä¸­æ£€ç´¢æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚
    """
    try:
        retriever = knowledge_manager.get_retriever()
        if retriever is None:
            return ["ğŸ“š çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ ç›¸å…³æ–‡æ¡£ã€‚æ‚¨å¯ä»¥ä¸Šä¼ PDFæ ¼å¼çš„ç•™å­¦ç”³è¯·æ¡ˆä¾‹ã€å­¦æ ¡ä»‹ç»ã€æ–‡ä¹¦æŒ‡å¯¼ç­‰æ–‡æ¡£æ¥ä¸°å¯ŒçŸ¥è¯†åº“ã€‚"]
        
        docs = retriever.invoke(query)
        if not docs:
            return ["ğŸ“ åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œå»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢è·å–æœ€æ–°ä¿¡æ¯ã€‚"]
        
        results = []
        for i, doc in enumerate(docs, 1):
            # è·å–æ–‡æ¡£æ¥æº
            source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
            source_name = os.path.basename(source) if source != 'æœªçŸ¥æ¥æº' else source
            
            results.append(f"ğŸ“– æ¥æº {i}: {source_name}\nå†…å®¹: {doc.page_content}\n")
        
        return results
        
    except Exception as e:
        return [f"âŒ çŸ¥è¯†åº“æ£€ç´¢å‡ºé”™: {str(e)}"]

@tool  
def get_knowledge_base_stats() -> str:
    """
    è·å–çŸ¥è¯†åº“çš„ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ–‡æ¡£æ•°é‡å’ŒçŠ¶æ€ã€‚
    å½“ç”¨æˆ·è¯¢é—®çŸ¥è¯†åº“çŠ¶æ€æˆ–æƒ³äº†è§£æœ‰å“ªäº›æ–‡æ¡£æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚
    """
    try:
        stats = knowledge_manager.get_knowledge_base_stats()
        
        result = f"""ğŸ“Š çŸ¥è¯†åº“çŠ¶æ€æŠ¥å‘Š:
        
ğŸ“ æ–‡æ¡£æ•°é‡: {stats['files_count']} ä¸ªæ–‡ä»¶
ğŸ§  å‘é‡åº“çŠ¶æ€: {'å·²å»ºç«‹' if stats['vector_store_exists'] else 'æœªå»ºç«‹'}

ğŸ“š å·²ä¸Šä¼ çš„æ–‡æ¡£:"""

        if stats['files']:
            for file in stats['files']:
                result += f"\n  â€¢ {file}"
        else:
            result += "\n  æš‚æ— æ–‡æ¡£"
            
        if not stats['vector_store_exists'] and stats['files_count'] > 0:
            result += "\n\nğŸ’¡ æç¤º: æ£€æµ‹åˆ°æœ‰æ–‡æ¡£ä½†å‘é‡åº“ä¸å­˜åœ¨ï¼Œå»ºè®®é‡æ–°æ„å»ºçŸ¥è¯†åº“ã€‚"
            
        return result
        
    except Exception as e:
        return f"âŒ è·å–çŸ¥è¯†åº“çŠ¶æ€å¤±è´¥: {str(e)}"
