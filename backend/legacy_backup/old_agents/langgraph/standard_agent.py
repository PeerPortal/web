#!/usr/bin/env python3
"""
ä½¿ç”¨LangGraphæ ‡å‡†æ¨¡å¼é‡æ„Agent Graph
"""

import time
from typing import Dict, Any, List, Optional, Annotated
from typing_extensions import TypedDict

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

from app.core.config import settings
from app.agents.langgraph.agent_tools import agent_tools
from app.agents.langgraph.query_classifier import query_classifier
from app.core.langsmith_config import (
    study_abroad_tracer,
    get_langsmith_callbacks,
    log_agent_metrics,
    is_langsmith_enabled
)

# ä½¿ç”¨æ ‡å‡†çš„LangGraph Stateå®šä¹‰
class State(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]

class StandardPlannerAgent:
    """ä½¿ç”¨LangGraphæ ‡å‡†æ¨¡å¼çš„AIç•™å­¦è§„åˆ’å¸ˆAgent"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            openai_api_key=settings.OPENAI_API_KEY,
            temperature=0.1
        )
        
        # ç»‘å®šå·¥å…·åˆ°LLM
        self.llm_with_tools = self.llm.bind_tools(agent_tools)
        
        # æ„å»ºLangGraph
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """æ„å»ºLangGraphæ‰§è¡Œå›¾"""
        # åˆ›å»ºçŠ¶æ€å›¾
        workflow = StateGraph(State)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("agent", self._call_model)
        workflow.add_node("tools", ToolNode(agent_tools))
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("agent")
        
        # æ·»åŠ æ¡ä»¶è¾¹
        workflow.add_conditional_edges(
            "agent",
            self._should_continue,
            {
                "tools": "tools",
                "end": END,
            }
        )
        
        # å·¥å…·æ‰§è¡Œåå›åˆ°agent
        workflow.add_edge("tools", "agent")
        
        # ç¼–è¯‘å›¾
        return workflow.compile(checkpointer=MemorySaver())
    
    def _call_model(self, state: State) -> Dict[str, Any]:
        """è°ƒç”¨æ¨¡å‹è¿›è¡Œæ¨ç†"""
        # è·å–ç”¨æˆ·æŸ¥è¯¢
        user_message = None
        for msg in reversed(state["messages"]):
            if hasattr(msg, 'content') and isinstance(msg.content, str):
                user_message = msg.content
                break
        
        # æŸ¥è¯¢åˆ†ç±»å’Œæ™ºèƒ½æç¤º
        enhanced_prompt = self._get_enhanced_system_prompt(user_message)
        
        messages = [{"role": "system", "content": enhanced_prompt}] + state["messages"]
        
        response = self.llm_with_tools.invoke(messages)
        print(f"ğŸ¤– æ¨¡å‹å“åº”: {type(response)}, å·¥å…·è°ƒç”¨: {len(response.tool_calls) if response.tool_calls else 0}")
        
        return {"messages": [response]}
    
    def _get_enhanced_system_prompt(self, user_query: str = None) -> str:
        """è·å–å¢å¼ºçš„ç³»ç»Ÿæç¤ºï¼ŒåŒ…å«æŸ¥è¯¢åˆ†æ"""
        base_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å‹å–„çš„AIç•™å­¦è§„åˆ’å¸ˆï¼Œåå«"å¯èˆªAI"ã€‚

ğŸ¯ ä½ çš„æ ¸å¿ƒèƒ½åŠ›ï¼š
- æ ¹æ®ç”¨æˆ·éœ€æ±‚æ™ºèƒ½é€‰æ‹©åˆé€‚çš„å·¥å…·æ¥è·å–ä¿¡æ¯
- ä¼˜å…ˆä»ç§æœ‰çŸ¥è¯†åº“è·å–ä¸“ä¸šçš„ç•™å­¦æŒ‡å¯¼ä¿¡æ¯
- å½“çŸ¥è¯†åº“æ— æ³•å›ç­”æ—¶ï¼Œä½¿ç”¨ç½‘ç»œæœç´¢è·å–æœ€æ–°ä¿¡æ¯
- æŸ¥è¯¢å¹³å°æ•°æ®åº“åŒ¹é…åˆé€‚çš„å¼•è·¯äººå’ŒæœåŠ¡
- æä¾›ä¸ªæ€§åŒ–çš„ç•™å­¦ç”³è¯·å»ºè®®å’Œè§„åˆ’

ï¿½ å·¥å…·ä½¿ç”¨ç­–ç•¥ï¼ˆä¸¥æ ¼éµå¾ªä¼˜å…ˆçº§ï¼‰ï¼š

STEP 1: æŸ¥è¯¢åˆ†æ
- æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦åŒ…å«: æ¡ˆä¾‹ã€GPAã€æ‰˜ç¦ã€GREã€å­¦æ ¡åç§°ã€æ¨èä¿¡ã€æ–‡ä¹¦ã€é¢è¯•ç­‰
- æ£€æŸ¥æ˜¯å¦æåˆ°: çŸ¥è¯†åº“ã€æ–‡æ¡£ã€ä¸Šä¼ çš„ã€æ ¹æ®ç­‰
- æ£€æŸ¥æ˜¯å¦è¯¢é—®: æ€ä¹ˆå†™ã€å¦‚ä½•å‡†å¤‡ã€æŠ€å·§ã€è¦ç‚¹ã€æ–¹æ³•ã€æ­¥éª¤ç­‰

STEP 2: å¼ºåˆ¶æ€§åˆ¤æ–­
å¦‚æœSTEP 1ä¸­ä»»ä¸€æ¡ä»¶æ»¡è¶³ï¼Œå¿…é¡»è°ƒç”¨ knowledge_base_retriever å·¥å…·

STEP 3: è¡¥å……æœç´¢  
ä»…å½“çŸ¥è¯†åº“æ— ç›¸å…³ç»“æœæ—¶ï¼Œæ‰è€ƒè™‘ web_search

ğŸ“‹ å…·ä½“ç¤ºä¾‹:
âœ… å¿…é¡»ä½¿ç”¨çŸ¥è¯†åº“:
- "æœ‰CMUç”³è¯·æ¡ˆä¾‹å—?" â†’ knowledge_base_retriever
- "GPA 3.8èƒ½ç”³è¯·å“ªäº›å­¦æ ¡?" â†’ knowledge_base_retriever  
- "æ¨èä¿¡æ€ä¹ˆå†™?" â†’ knowledge_base_retriever
- "æ–‡ä¹¦å†™ä½œæŠ€å·§" â†’ knowledge_base_retriever
- "é¢è¯•å‡†å¤‡è¦ç‚¹" â†’ knowledge_base_retriever

âŒ å¯ä»¥ä½¿ç”¨ç½‘ç»œæœç´¢:
- "2024å¹´æœ€æ–°æ’å" â†’ web_search
- "æœ€æ–°æ”¿ç­–å˜åŒ–" â†’ web_search

âš ï¸ è¿åè§„åˆ™å°†å¯¼è‡´å›ç­”ä¸å‡†ç¡®!"""
        
        # å¦‚æœæœ‰ç”¨æˆ·æŸ¥è¯¢ï¼Œæ·»åŠ æ™ºèƒ½åˆ†æ
        if user_query:
            classification = query_classifier.classify_query(user_query)
            analysis_text = f"""

ğŸ” å½“å‰æŸ¥è¯¢åˆ†æ:
æŸ¥è¯¢: "{user_query}"
æ¨èå·¥å…·: {classification['recommended_tool']}
ç½®ä¿¡åº¦: {classification['confidence']:.2f}
åŸå› : {', '.join(classification['reasons'])}

âš¡ åŸºäºåˆ†æï¼Œä½ åº”è¯¥ä¼˜å…ˆä½¿ç”¨ {classification['recommended_tool']} å·¥å…·!"""
            
            base_prompt += analysis_text
        
        return base_prompt
    
    def _should_continue(self, state: State) -> str:
        """å†³å®šæ˜¯å¦ç»§ç»­æ‰§è¡Œå·¥å…·"""
        messages = state['messages']
        last_message = messages[-1]
        
        # å¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯æœ‰å·¥å…·è°ƒç”¨ï¼Œç»§ç»­æ‰§è¡Œå·¥å…·
        if last_message.tool_calls:
            return "tools"
        
        # å¦åˆ™ç»“æŸ
        return "end"
    
    async def ainvoke(self, input_data: dict) -> dict:
        """å¼‚æ­¥è°ƒç”¨Agent"""
        user_id = input_data.get("user_id", "anonymous")
        input_message = input_data["input"]
        start_time = time.time()
        
        try:
            # å‡†å¤‡åˆå§‹çŠ¶æ€
            initial_state = {
                "messages": [HumanMessage(content=input_message)]
            }
            
            # æ‰§è¡Œå›¾
            config = {"configurable": {"thread_id": user_id}}
            final_state = await self.graph.ainvoke(initial_state, config)
            
            # æå–ç»“æœ
            last_message = final_state["messages"][-1]
            output = last_message.content if hasattr(last_message, 'content') else str(last_message)
            
            execution_time = time.time() - start_time
            
            return {
                "output": output,
                "session_id": user_id,
                "metadata": {
                    "execution_time": execution_time,
                    "messages_count": len(final_state["messages"])
                }
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            return {
                "output": f"æŠ±æ­‰ï¼Œç³»ç»Ÿå‡ºç°äº†é”™è¯¯ï¼š{str(e)}",
                "session_id": user_id,
                "metadata": {
                    "execution_time": execution_time,
                    "error": str(e)
                }
            }

# åˆ›å»ºå…¨å±€å®ä¾‹
_standard_agent_instance = None

def get_standard_agent() -> StandardPlannerAgent:
    """è·å–æ ‡å‡†Agentå®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _standard_agent_instance
    if _standard_agent_instance is None:
        _standard_agent_instance = StandardPlannerAgent()
    return _standard_agent_instance
